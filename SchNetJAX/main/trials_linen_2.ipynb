{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from jax import random\n",
    "import numpy as np\n",
    "#from flax import nnx\n",
    "from typing import Dict\n",
    "\n",
    "import jax_dataloader as jdl\n",
    "from functools import partial\n",
    "#######\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data#, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterDataset(Dataset):\n",
    "    \"\"\"Water molecules dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, npz_file):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            npz_file (string): Path to the npz file with data.\n",
    "        \"\"\"\n",
    "        data = np.load(npz_file)\n",
    "        self.E = data[\"E\"]\n",
    "        self.Z = data[\"z\"]\n",
    "        self.R = data[\"R\"]\n",
    "        self.E_norm = (data[\"E\"] - np.mean(data[\"E\"]))/np.std(data[\"E\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.E)\n",
    "    \n",
    "    def energy_std(self):\n",
    "        return np.std(self.E)\n",
    "    \n",
    "    def energy_mean(self):\n",
    "        return np.mean(self.E)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {'positions': self.R[idx], 'atom_types': self.Z, 'energy': self.E_norm[idx]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_dataset = WaterDataset(\"data_water.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_loader = jdl.DataLoader(water_dataset, backend='pytorch', batch_size=5, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(water_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73456615],\n",
       "       [ 3.46393816],\n",
       "       [-0.0459205 ],\n",
       "       [-0.75315944],\n",
       "       [ 1.30973884]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"energy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.767767751286364)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_dataset.energy_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEmbedding(nn.Module):\n",
    "    num_atom_types: int\n",
    "    embedding_dim: int\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(\n",
    "            num_embeddings=self.num_atom_types,\n",
    "            features=self.embedding_dim\n",
    "        )\n",
    "\n",
    "    def __call__(self, atom_types):\n",
    "        ord_atom_types = jnp.unique(atom_types, size=self.num_atom_types)\n",
    "        mask = jnp.asarray([jnp.where(ord_atom_types==elem, size=1) for elem in atom_types]).squeeze()\n",
    "        return jnp.array(self.embedding(mask)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a positions thing, r = (r_1, ..., r_n), returns\n",
    "# a matrix with the relative distance among them\n",
    "# (d_ij)_ij = |r_i - r_j|^2\n",
    "\n",
    "class R_distances(nn.Module):\n",
    "\n",
    "    def __call__(self, R):\n",
    "        Rij = jnp.array([[r_i - r_j for r_j in R] for r_i in R])\n",
    "        d_ij = jnp.linalg.norm(Rij, axis=-1)\n",
    "        return d_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunctions(nn.Module):\n",
    "    rbf_min: float\n",
    "    rbf_max: float\n",
    "    n_rbf: int\n",
    "    gamma: float = 10\n",
    "\n",
    "    def setup(self):\n",
    "        self.centers = jnp.linspace(self.rbf_min, self.rbf_max, self.n_rbf).reshape(1, -1)\n",
    "\n",
    "    def __call__(self, d_ij):\n",
    "        diff = d_ij[..., None] - self.centers\n",
    "        return jnp.exp(-self.gamma * jnp.pow(diff, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU function\n",
    "\n",
    "class relu_layer(nn.Module):\n",
    "    def __call__(self, x):\n",
    "        return nn.relu(x)\n",
    "\n",
    "# Shift Softplus layer\n",
    "\n",
    "class ssp_layer(nn.Module):\n",
    "    def __call__(self, x):\n",
    "        return jnp.log(0.5 * jnp.exp(x) + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_generator(nn.Module):\n",
    "    atom_embeddings_dim: int\n",
    "    rbf_min: float\n",
    "    rbf_max: float\n",
    "    n_rbf: int\n",
    "    activation: nn.Module = ssp_layer\n",
    "\n",
    "    def setup(self):\n",
    "        self.rbf = RadialBasisFunctions(\n",
    "            self.rbf_min, \n",
    "            self.rbf_max, \n",
    "            self.n_rbf\n",
    "            )\n",
    "        \n",
    "        self.w_layers = nn.Sequential([\n",
    "            nn.Dense(self.n_rbf, self.atom_embeddings_dim),\n",
    "            self.activation(),\n",
    "            nn.Dense(self.atom_embeddings_dim, self.atom_embeddings_dim),\n",
    "            self.activation()\n",
    "            ])\n",
    "\n",
    "    def __call__(self, d_ij):\n",
    "        rbfs = self.rbf(d_ij)\n",
    "        Wij = self.w_layers(rbfs)\n",
    "        return Wij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CfConv(nn.Module):\n",
    "    \n",
    "    def __call__(self, X, fij):\n",
    "        ones = jnp.ones(shape=fij.shape)\n",
    "        jax.lax.fori_loop(0, len(ones), lambda i, ones_i: ones_i.at[i,i].set(jnp.zeros(shape=ones_i[i,i].shape)), ones)\n",
    "        #filters = jnp.sum(fij, axis = 1)\n",
    "        X_j = X[None, :, :]\n",
    "        Xij = X_j * fij\n",
    "        #jnp.sum(Xij, axis = 0, where = ones)\n",
    "        return X + jnp.sum(Xij, axis=1, where = ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionBlock(nn.Module):\n",
    "    atom_embeddings_dim: int\n",
    "    rbf_min: float\n",
    "    rbf_max: float\n",
    "    n_rbf: int\n",
    "    activation: nn.Module = ssp_layer\n",
    "\n",
    "    def setup(self):\n",
    "        self.in_atom_wise = nn.Dense(\n",
    "            self.atom_embeddings_dim,\n",
    "            self.atom_embeddings_dim\n",
    "            )\n",
    "        \n",
    "        self.out_atom_wise = nn.Sequential([\n",
    "            nn.Dense(self.atom_embeddings_dim, self.atom_embeddings_dim),\n",
    "            self.activation(),\n",
    "            nn.Dense(self.atom_embeddings_dim, self.atom_embeddings_dim)\n",
    "            ])\n",
    "\n",
    "        self.filters = filter_generator(\n",
    "            self.atom_embeddings_dim,\n",
    "            self.rbf_min,\n",
    "            self.rbf_max,\n",
    "            self.n_rbf\n",
    "            )\n",
    "\n",
    "        self.cf_conv = CfConv()\n",
    "\n",
    "        self.distances = R_distances()\n",
    "\n",
    "    def __call__(self, X, R):\n",
    "        X_in = self.in_atom_wise(X)\n",
    "        R_distances = self.distances(R)\n",
    "        fils = self.filters(R_distances)\n",
    "        X_conv = self.cf_conv(X_in, fils)\n",
    "        V = self.out_atom_wise(X_conv)\n",
    "        return X + V, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchNet(nn.Module):\n",
    "    atom_embedding_dim: int = 64\n",
    "    atom_wise_out: int = 32\n",
    "    n_interactions: int = 3\n",
    "    n_atom_types: int = 2\n",
    "    rbf_min: float = 0.\n",
    "    rbf_max: float = 30.\n",
    "    n_rbf: int = 300\n",
    "    activation: nn.Module = ssp_layer\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = AtomEmbedding(\n",
    "            self.n_atom_types,\n",
    "            self.atom_embedding_dim\n",
    "        )\n",
    "\n",
    "        self.interaction = InteractionBlock(self.atom_embedding_dim, self.rbf_min, self.rbf_max, self.n_rbf, self.activation)\n",
    "\n",
    "        self.interactions = nn.Sequential([\n",
    "            InteractionBlock(self.atom_embedding_dim, self.rbf_min, self.rbf_max, self.n_rbf, self.activation)\n",
    "            for _ in range(self.n_interactions)\n",
    "        ])\n",
    "\n",
    "        self.output_layers = nn.Sequential([\n",
    "            nn.Dense(self.atom_embedding_dim, self.atom_wise_out),\n",
    "            self.activation(),\n",
    "            nn.Dense(self.atom_wise_out, 1)\n",
    "        ])\n",
    "\n",
    "    @partial(jax.vmap, in_axes = (None, 0, 0), out_axes = 0)\n",
    "    def __call__(self, Z, R):\n",
    "        X = self.embedding(Z)\n",
    "        X_interacted = X\n",
    "        X_interacted, R = self.interactions(X = X_interacted, R = R)\n",
    "        atom_outputs = self.output_layers(X_interacted)\n",
    "        predicted_energies = jnp.sum(atom_outputs)\n",
    "        return predicted_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "sch = SchNet()\n",
    "Z = batch[\"atom_types\"]\n",
    "R = batch[\"positions\"]\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "params_sch = sch.init(key2, Z, R)\n",
    "energies = sch.apply(params_sch, Z, R)\n",
    "print(energies.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Training ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def mse(params, model, Z, R, E):\n",
    "    E_pred = model.apply(params, Z, R)\n",
    "    mse_v = jnp.inner(E - E_pred, E - E_pred) / 2.0\n",
    "    return jnp.mean(mse_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad_fn = jax.value_and_grad(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "    params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads\n",
    "    )\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training shit\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.005\n",
    "batchsize = 1_000\n",
    "epochs = 1_000\n",
    "freq = 1\n",
    "\n",
    "model = SchNet(atom_embedding_dim=16)\n",
    "\n",
    "water_dataset = WaterDataset(\"data_water.npz\")\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(water_dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = jdl.DataLoader(train_dataset, backend='pytorch', batch_size=batchsize, shuffle=True, drop_last=False)\n",
    "test_loader = jdl.DataLoader(test_dataset, backend=\"pytorch\", batch_size=batchsize, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss in epoch 0 | nan\n",
      "Test error in epoch 0 | nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     12\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43matom_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax_dataloader/loaders/torch.py:20\u001b[0m, in \u001b[0;36m_numpy_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy_collate\u001b[39m(batch):\n\u001b[0;32m---> 20\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tree_map(np\u001b[38;5;241m.\u001b[39masarray, \u001b[43mtorch_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m         {\n\u001b[0;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:285\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_loader))\n",
    "Z = batch[\"atom_types\"]\n",
    "R = batch[\"positions\"]\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "params = model.init(key2, Z, R)\n",
    "total_loss = 0.\n",
    "test_loss = 0.\n",
    "learning_rate=1E-5\n",
    "for epoch in range(epochs):\n",
    "    j = 0\n",
    "    total_loss = 0.\n",
    "    test_loss = 0.\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        Z = batch[\"atom_types\"]\n",
    "        R = batch[\"positions\"]\n",
    "        E = batch[\"energy\"]\n",
    "        loss, grads = loss_grad_fn(params, model, Z, R, E)\n",
    "        params = update_params(params, learning_rate, grads)\n",
    "        total_loss += loss\n",
    "        j += 1\n",
    "    print(f\"Train loss in epoch {epoch} | {total_loss / j}\")\n",
    "\n",
    "    j = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        Z = batch[\"atom_types\"]\n",
    "        R = batch[\"positions\"]\n",
    "        E = batch[\"energy\"]\n",
    "        test_loss += mse(params, model, Z, R, E)\n",
    "        j += 1\n",
    "    print(f\"Test error in epoch {epoch} | {test_loss / j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(371.93015, dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SchNet()\n",
    "Z = batch[\"atom_types\"]\n",
    "R = batch[\"positions\"]\n",
    "E = batch[\"energy\"]\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "params = model.init(key2, Z, R)\n",
    "mse(params, Z, R, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "def create_train_step(key, Z, R, model, optimizer):\n",
    "    params = model.init(key, Z, R)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jax.jit\n",
    "    def loss_fn(params, Z_batched, R_batched, E_batched):\n",
    "        pred_energies = model.apply(params, Z_batched, R_batched)\n",
    "        return jnp.mean(jnp.pow(pred_energies - E, 2))\n",
    "    \n",
    "    @jax.jit\n",
    "    def train_step(params, opt_state, Z, R, E):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, Z, R, E)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss\n",
    "    \n",
    "    @jax.jit\n",
    "    def test_fn(params, Z, R, E):\n",
    "        pred_energies = model.apply(params, Z, R)\n",
    "        return jnp.mean(jnp.pow(pred_energies - E, 2))\n",
    "\n",
    "    return train_step, test_fn, params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training shit\n",
    "\n",
    "import optax\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.005\n",
    "batchsize = 5_000\n",
    "\n",
    "model = SchNet(atom_embedding_dim=32, n_rbf=30)\n",
    "\n",
    "water_dataset = WaterDataset(\"data_water.npz\")\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(water_dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = jdl.DataLoader(train_dataset, backend='pytorch', batch_size=batchsize, shuffle=True, drop_last=False)\n",
    "test_loader = jdl.DataLoader(test_dataset, backend=\"pytorch\", batch_size=batchsize, drop_last=False)\n",
    "\n",
    "batch = next(iter(water_loader))\n",
    "\n",
    "Z = batch[\"atom_types\"]\n",
    "R = batch[\"positions\"]\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=learning_rate)\n",
    "\n",
    "train_step, test_fn, params, opt_state = create_train_step(key2, Z, R, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | step 10 | loss: 12.767768859863281\n",
      "Test error in epoch 0 | loss: 1.0893431901931763\n",
      "epoch 1 | step 10 | loss: 1.4749388694763184\n",
      "Test error in epoch 1 | loss: 1.068005084991455\n",
      "epoch 2 | step 10 | loss: 1.1945760250091553\n",
      "Test error in epoch 2 | loss: 0.9841251969337463\n",
      "epoch 3 | step 10 | loss: 1.1398496627807617\n",
      "Test error in epoch 3 | loss: 0.9777716398239136\n",
      "epoch 4 | step 10 | loss: 1.1300773620605469\n",
      "Test error in epoch 4 | loss: 0.9771242737770081\n",
      "epoch 5 | step 10 | loss: 1.1282262802124023\n",
      "Test error in epoch 5 | loss: 0.9765509366989136\n",
      "epoch 6 | step 10 | loss: 1.127841591835022\n",
      "Test error in epoch 6 | loss: 0.9767917394638062\n",
      "epoch 7 | step 10 | loss: 1.1277782917022705\n",
      "Test error in epoch 7 | loss: 0.976589560508728\n",
      "epoch 8 | step 10 | loss: 1.1277592182159424\n",
      "Test error in epoch 8 | loss: 0.9766359329223633\n",
      "epoch 9 | step 10 | loss: 1.1277509927749634\n",
      "Test error in epoch 9 | loss: 0.9765997529029846\n",
      "epoch 10 | step 10 | loss: 1.127746343612671\n",
      "Test error in epoch 10 | loss: 0.9766208529472351\n",
      "epoch 11 | step 10 | loss: 1.127741813659668\n",
      "Test error in epoch 11 | loss: 0.9766045808792114\n",
      "epoch 12 | step 10 | loss: 1.1277379989624023\n",
      "Test error in epoch 12 | loss: 0.976600706577301\n",
      "epoch 13 | step 10 | loss: 1.1277347803115845\n",
      "Test error in epoch 13 | loss: 0.97660231590271\n",
      "epoch 14 | step 10 | loss: 1.1277323961257935\n",
      "Test error in epoch 14 | loss: 0.9766002297401428\n",
      "epoch 15 | step 10 | loss: 1.1277300119400024\n",
      "Test error in epoch 15 | loss: 0.9765994548797607\n",
      "epoch 16 | step 10 | loss: 1.1277282238006592\n",
      "Test error in epoch 16 | loss: 0.9765959978103638\n",
      "epoch 17 | step 10 | loss: 1.1277263164520264\n",
      "Test error in epoch 17 | loss: 0.9765951633453369\n",
      "epoch 18 | step 10 | loss: 1.1277247667312622\n",
      "Test error in epoch 18 | loss: 0.9765920639038086\n",
      "epoch 19 | step 10 | loss: 1.1277236938476562\n",
      "Test error in epoch 19 | loss: 0.9765925407409668\n",
      "epoch 20 | step 10 | loss: 1.1277227401733398\n",
      "Test error in epoch 20 | loss: 0.9765938520431519\n",
      "epoch 21 | step 10 | loss: 1.1277220249176025\n",
      "Test error in epoch 21 | loss: 0.9765900373458862\n",
      "epoch 22 | step 10 | loss: 1.1277210712432861\n",
      "Test error in epoch 22 | loss: 0.9765917062759399\n",
      "epoch 23 | step 10 | loss: 1.127720832824707\n",
      "Test error in epoch 23 | loss: 0.976588785648346\n",
      "epoch 24 | step 10 | loss: 1.1277202367782593\n",
      "Test error in epoch 24 | loss: 0.9765906929969788\n",
      "epoch 25 | step 10 | loss: 1.1277199983596802\n",
      "Test error in epoch 25 | loss: 0.9765903353691101\n",
      "epoch 26 | step 10 | loss: 1.1277196407318115\n",
      "Test error in epoch 26 | loss: 0.9765923023223877\n",
      "epoch 27 | step 10 | loss: 1.127719521522522\n",
      "Test error in epoch 27 | loss: 0.9765896201133728\n",
      "epoch 28 | step 10 | loss: 1.1277192831039429\n",
      "Test error in epoch 28 | loss: 0.9765873551368713\n",
      "epoch 29 | step 10 | loss: 1.1277190446853638\n",
      "Test error in epoch 29 | loss: 0.9765891432762146\n",
      "epoch 30 | step 10 | loss: 1.1277188062667847\n",
      "Test error in epoch 30 | loss: 0.9765899777412415\n",
      "epoch 31 | step 10 | loss: 1.1277188062667847\n",
      "Test error in epoch 31 | loss: 0.9765889644622803\n",
      "epoch 32 | step 10 | loss: 1.1277186870574951\n",
      "Test error in epoch 32 | loss: 0.9765887260437012\n",
      "epoch 33 | step 10 | loss: 1.1277186870574951\n",
      "Test error in epoch 33 | loss: 0.9765886664390564\n",
      "epoch 34 | step 10 | loss: 1.1277186870574951\n",
      "Test error in epoch 34 | loss: 0.9765884280204773\n",
      "epoch 35 | step 10 | loss: 1.1277186870574951\n",
      "Test error in epoch 35 | loss: 0.9765890836715698\n",
      "epoch 36 | step 10 | loss: 1.1277185678482056\n",
      "Test error in epoch 36 | loss: 0.9765892028808594\n",
      "epoch 37 | step 10 | loss: 1.1277185678482056\n",
      "Test error in epoch 37 | loss: 0.9765896201133728\n",
      "epoch 38 | step 10 | loss: 1.127718448638916\n",
      "Test error in epoch 38 | loss: 0.9765881299972534\n",
      "epoch 39 | step 10 | loss: 1.1277185678482056\n",
      "Test error in epoch 39 | loss: 0.9765886664390564\n",
      "epoch 40 | step 10 | loss: 1.1277183294296265\n",
      "Test error in epoch 40 | loss: 0.9765883684158325\n",
      "epoch 41 | step 10 | loss: 1.1277185678482056\n",
      "Test error in epoch 41 | loss: 0.9765896201133728\n",
      "epoch 42 | step 10 | loss: 1.127718448638916\n",
      "Test error in epoch 42 | loss: 0.976588249206543\n",
      "epoch 43 | step 10 | loss: 1.1277183294296265\n",
      "Test error in epoch 43 | loss: 0.9765893220901489\n",
      "epoch 44 | step 10 | loss: 1.1277183294296265\n",
      "Test error in epoch 44 | loss: 0.9765880703926086\n",
      "epoch 45 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 45 | loss: 0.9765891432762146\n",
      "epoch 46 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 46 | loss: 0.9765886664390564\n",
      "epoch 47 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 47 | loss: 0.9765889048576355\n",
      "epoch 48 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 48 | loss: 0.9765875339508057\n",
      "epoch 49 | step 10 | loss: 1.1277183294296265\n",
      "Test error in epoch 49 | loss: 0.9765884876251221\n",
      "epoch 50 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 50 | loss: 0.9765883088111877\n",
      "epoch 51 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 51 | loss: 0.976588249206543\n",
      "epoch 52 | step 10 | loss: 1.1277183294296265\n",
      "Test error in epoch 52 | loss: 0.9765886068344116\n",
      "epoch 53 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 53 | loss: 0.9765885472297668\n",
      "epoch 54 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 54 | loss: 0.9765884280204773\n",
      "epoch 55 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 55 | loss: 0.9765889048576355\n",
      "epoch 56 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 56 | loss: 0.976589024066925\n",
      "epoch 57 | step 10 | loss: 1.1277180910110474\n",
      "Test error in epoch 57 | loss: 0.9765884280204773\n",
      "epoch 58 | step 10 | loss: 1.1277180910110474\n",
      "Test error in epoch 58 | loss: 0.9765878319740295\n",
      "epoch 59 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 59 | loss: 0.9765880703926086\n",
      "epoch 60 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 60 | loss: 0.976588785648346\n",
      "epoch 61 | step 10 | loss: 1.1277180910110474\n",
      "Test error in epoch 61 | loss: 0.9765887260437012\n",
      "epoch 62 | step 10 | loss: 1.1277180910110474\n",
      "Test error in epoch 62 | loss: 0.9765886068344116\n",
      "epoch 63 | step 10 | loss: 1.1277180910110474\n",
      "Test error in epoch 63 | loss: 0.9765880107879639\n",
      "epoch 64 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 64 | loss: 0.9765889048576355\n",
      "epoch 65 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 65 | loss: 0.9765879511833191\n",
      "epoch 66 | step 10 | loss: 1.1277183294296265\n",
      "Test error in epoch 66 | loss: 0.976588785648346\n",
      "epoch 67 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 67 | loss: 0.9765875935554504\n",
      "epoch 68 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 68 | loss: 0.976589024066925\n",
      "epoch 69 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 69 | loss: 0.9765891432762146\n",
      "epoch 70 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 70 | loss: 0.976588785648346\n",
      "epoch 71 | step 10 | loss: 1.1277180910110474\n",
      "Test error in epoch 71 | loss: 0.9765892028808594\n",
      "epoch 72 | step 10 | loss: 1.127718210220337\n",
      "Test error in epoch 72 | loss: 0.9765888452529907\n",
      "epoch 73 | step 10 | loss: 1.1277179718017578\n",
      "Test error in epoch 73 | loss: 0.9765889048576355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m   total_loss, total_mse, total_kl \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43matom_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax_dataloader/loaders/torch.py:20\u001b[0m, in \u001b[0;36m_numpy_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy_collate\u001b[39m(batch):\n\u001b[0;32m---> 20\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tree_map(np\u001b[38;5;241m.\u001b[39masarray, \u001b[43mtorch_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m         {\n\u001b[0;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:285\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "freq = 10\n",
    "epochs = 1_000\n",
    "\n",
    "num_test = len(test_dataset)\n",
    "for epoch in range(epochs):\n",
    "  total_loss, total_mse, total_kl = 0.0, 0.0, 0.0\n",
    "  for i, batch in enumerate(train_loader):\n",
    "\n",
    "    Z = batch[\"atom_types\"]\n",
    "    R = batch[\"positions\"]\n",
    "    E = batch[\"energy\"]\n",
    "\n",
    "    params, opt_state, loss = train_step(params, opt_state, Z, R, E)\n",
    "\n",
    "    total_loss += loss\n",
    "\n",
    "    if i > 0 and not i % freq:\n",
    "      print(f\"epoch {epoch} | step {i} | loss: {total_loss / freq}\")\n",
    "      total_loss = 0.\n",
    "\n",
    "  test_loss = 0.\n",
    "  j = 0\n",
    "  for i, batch in enumerate(test_loader):\n",
    "    Z = batch[\"atom_types\"]\n",
    "    R = batch[\"positions\"]\n",
    "    E = batch[\"energy\"]\n",
    "    test_loss += test_fn(params, Z, R, E)\n",
    "    j += 1\n",
    "  print(f\"Test error in epoch {epoch} | loss: {test_loss / j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_params(params, loss_fn):\n",
    "    losses, grads = jax.value_and_grad(loss_fn)(params,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax+torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
